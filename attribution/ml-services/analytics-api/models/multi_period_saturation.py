"""
Multi-Period Saturation Model - Enhanced CPA/Traffic Cost Prediction
UnMoGrowP Attribution Platform

–ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å 4 –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏:
1. Last 7 days - –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
2. Last 14 days - —Å—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
3. Last 30 days - –º–µ—Å—è—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
4. Adaptive period - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞–º–ø–∞–Ω–∏–∏

–ö–∞–∂–¥—ã–π –ø–µ—Ä–∏–æ–¥ –¥–∞–µ—Ç —Å–≤–æ–π –ø—Ä–æ–≥–Ω–æ–∑, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–∏—Ç –≤—Å–µ 4 –≤–∞—Ä–∏–∞–Ω—Ç–∞ + ensemble.
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from scipy.optimize import curve_fit
import xgboost as xgb
from sklearn.metrics import mean_absolute_percentage_error
import logging

logger = logging.getLogger(__name__)

@dataclass
class SaturationPrediction:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞"""
    period_name: str
    period_days: int
    data_points: int
    confidence: float  # 0-1

    # Curve parameters
    max_cpa: float
    steepness: float
    inflection_point: float

    # –ü—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö spend levels
    spend_trajectory: List[Dict]

    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    mape: float
    r_squared: float
    fit_quality: str  # 'excellent', 'good', 'fair', 'poor'

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    optimal_spend: float
    saturation_warning: Optional[str]

@dataclass
class EnsemblePrediction:
    """–§–∏–Ω–∞–ª—å–Ω—ã–π ensemble –ø—Ä–æ–≥–Ω–æ–∑"""
    consensus_trajectory: List[Dict]
    confidence_intervals: List[Dict]

    # –í–µ—Å–∞ –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –≤ ensemble
    period_weights: Dict[str, float]

    # Ensemble –º–µ—Ç—Ä–∏–∫–∏
    ensemble_confidence: float
    prediction_variance: float

    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    recommended_spend: float
    risk_level: str  # 'low', 'medium', 'high'

class MultiPeriodSaturationModel:
    """
    Enhanced Saturation Model —Å 4 –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏
    """

    def __init__(self):
        self.periods = {
            'short_term': 7,    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
            'medium_term': 14,  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 14 –¥–Ω–µ–π
            'long_term': 30,    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
            'adaptive': None    # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è
        }

        # XGBoost –¥–ª—è refinement –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        self.refinement_models = {}
        for period in self.periods.keys():
            self.refinement_models[period] = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )

        # Ensemble meta-model
        self.ensemble_model = xgb.XGBRegressor(
            n_estimators=50,
            max_depth=4,
            learning_rate=0.2,
            random_state=42
        )

        self.min_data_points = 5  # –ú–∏–Ω–∏–º—É–º —Ç–æ—á–µ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    def predict_multi_period(self, campaign_id: str, target_spends: List[float]) -> Dict:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö 4 –ø–µ—Ä–∏–æ–¥–æ–≤

        Returns:
        {
            'predictions_by_period': {
                'short_term': SaturationPrediction,
                'medium_term': SaturationPrediction,
                'long_term': SaturationPrediction,
                'adaptive': SaturationPrediction
            },
            'ensemble': EnsemblePrediction,
            'metadata': {...}
        }
        """

        logger.info(f"Starting multi-period prediction for campaign {campaign_id}")

        results = {
            'campaign_id': campaign_id,
            'prediction_timestamp': datetime.now().isoformat(),
            'target_spends': target_spends,
            'predictions_by_period': {},
            'ensemble': None,
            'metadata': {}
        }

        # 1. –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        historical_data = self._get_historical_data(campaign_id)

        if len(historical_data) < self.min_data_points:
            logger.warning(f"Insufficient data for campaign {campaign_id}: {len(historical_data)} points")
            return self._insufficient_data_response(campaign_id, target_spends)

        # 2. –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        period_predictions = {}

        for period_name, period_days in self.periods.items():
            if period_name == 'adaptive':
                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
                optimal_days = self._find_optimal_period(historical_data, campaign_id)
                period_data = self._filter_by_period(historical_data, optimal_days)
                period_days = optimal_days
            else:
                period_data = self._filter_by_period(historical_data, period_days)

            if len(period_data) >= self.min_data_points:
                prediction = self._predict_single_period(
                    period_data,
                    period_name,
                    period_days,
                    target_spends,
                    campaign_id
                )
                period_predictions[period_name] = prediction
            else:
                logger.warning(f"Insufficient data for {period_name} period: {len(period_data)} points")

        results['predictions_by_period'] = period_predictions

        # 3. –°–æ–∑–¥–∞–µ–º ensemble –ø—Ä–æ–≥–Ω–æ–∑
        if len(period_predictions) >= 2:
            ensemble = self._create_ensemble(period_predictions, target_spends)
            results['ensemble'] = ensemble

        # 4. –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        results['metadata'] = self._generate_metadata(historical_data, period_predictions)

        logger.info(f"Multi-period prediction completed for campaign {campaign_id}")
        return results

    def _predict_single_period(self, data: pd.DataFrame, period_name: str,
                             period_days: int, target_spends: List[float],
                             campaign_id: str) -> SaturationPrediction:
        """–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞"""

        logger.debug(f"Predicting for {period_name} period ({period_days} days, {len(data)} points)")

        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        spend_values = data['ad_spend'].values
        cpi_values = data['cost_per_install'].values

        # 2. Fit logistic curve: CPI = L / (1 + exp(-k * (spend - x0)))
        try:
            curve_params = self._fit_logistic_curve(spend_values, cpi_values)
            max_cpa, steepness, inflection_point = curve_params
        except Exception as e:
            logger.warning(f"Curve fitting failed for {period_name}: {e}")
            # Fallback to linear trend
            max_cpa, steepness, inflection_point = self._fit_linear_fallback(spend_values, cpi_values)

        # 3. ML refinement —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏
        contextual_features = self._extract_contextual_features(data, campaign_id)
        base_predictions = [self._logistic_function(spend, max_cpa, steepness, inflection_point)
                          for spend in target_spends]

        # XGBoost –∫–æ—Ä—Ä–µ–∫—Ü–∏—è
        if period_name in self.refinement_models and len(data) > 10:
            ml_adjustments = self._apply_ml_refinement(
                contextual_features, period_name, target_spends
            )
            refined_predictions = [base * (1 + adj) for base, adj in zip(base_predictions, ml_adjustments)]
        else:
            refined_predictions = base_predictions
            ml_adjustments = [0.0] * len(target_spends)

        # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        predicted_cpi = [self._logistic_function(spend, max_cpa, steepness, inflection_point)
                        for spend in spend_values]
        mape = mean_absolute_percentage_error(cpi_values, predicted_cpi)
        r_squared = self._calculate_r_squared(cpi_values, predicted_cpi)

        # 5. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ fit
        fit_quality = self._assess_fit_quality(mape, r_squared, len(data))

        # 6. –§–æ—Ä–º–∏—Ä—É–µ–º trajectory
        spend_trajectory = []
        for i, spend in enumerate(target_spends):
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            saturation_level = self._calculate_saturation_level(spend, max_cpa, steepness, inflection_point)

            spend_trajectory.append({
                'spend': spend,
                'predicted_cpi': refined_predictions[i],
                'base_prediction': base_predictions[i],
                'ml_adjustment': ml_adjustments[i],
                'saturation_level': saturation_level,
                'efficiency_rating': self._rate_efficiency(saturation_level),
                'risk_level': self._assess_risk(saturation_level, refined_predictions[i])
            })

        # 7. –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π spend
        optimal_spend = self._find_optimal_spend(spend_trajectory)

        # 8. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        saturation_warning = self._generate_saturation_warning(spend_trajectory, period_name)

        # 9. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º confidence
        confidence = self._calculate_confidence(mape, r_squared, len(data), period_days)

        return SaturationPrediction(
            period_name=period_name,
            period_days=period_days,
            data_points=len(data),
            confidence=confidence,
            max_cpa=max_cpa,
            steepness=steepness,
            inflection_point=inflection_point,
            spend_trajectory=spend_trajectory,
            mape=mape,
            r_squared=r_squared,
            fit_quality=fit_quality,
            optimal_spend=optimal_spend,
            saturation_warning=saturation_warning
        )

    def _find_optimal_period(self, historical_data: pd.DataFrame, campaign_id: str) -> int:
        """
        –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        –ö—Ä–∏—Ç–µ—Ä–∏–∏:
        - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
        - –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞ (–Ω–µ —Å–ª–∏—à–∫–æ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ)
        - –°–≤–µ–∂–µ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö (–Ω–µ —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä—ã–µ)
        - –ö–∞—á–µ—Å—Ç–≤–æ fit (–ª—É—á—à–∏–π R¬≤)
        """

        candidate_periods = [7, 14, 21, 30, 45, 60, 90]
        period_scores = {}

        for days in candidate_periods:
            period_data = self._filter_by_period(historical_data, days)

            if len(period_data) < self.min_data_points:
                continue

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 1: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)
            data_score = min(len(period_data) / 20, 1.0)  # Optimal 20+ points

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 2: –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞
            stability_score = self._calculate_trend_stability(period_data)

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 3: –°–≤–µ–∂–µ—Å—Ç—å (–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ)
            freshness_score = 1.0 / (1 + days / 30)  # Decay —Å 30-–¥–Ω–µ–≤–Ω—ã–º –ø–æ–ª–æ–≤–∏–Ω–Ω—ã–º –ø–µ—Ä–∏–æ–¥–æ–º

            # –ö—Ä–∏—Ç–µ—Ä–∏–π 4: –ö–∞—á–µ—Å—Ç–≤–æ fit
            try:
                spend_values = period_data['ad_spend'].values
                cpi_values = period_data['cost_per_install'].values
                curve_params = self._fit_logistic_curve(spend_values, cpi_values)

                predicted_cpi = [self._logistic_function(spend, *curve_params) for spend in spend_values]
                r_squared = self._calculate_r_squared(cpi_values, predicted_cpi)
                fit_score = max(r_squared, 0)
            except:
                fit_score = 0

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            total_score = (
                data_score * 0.25 +
                stability_score * 0.25 +
                freshness_score * 0.25 +
                fit_score * 0.25
            )

            period_scores[days] = {
                'total_score': total_score,
                'data_score': data_score,
                'stability_score': stability_score,
                'freshness_score': freshness_score,
                'fit_score': fit_score,
                'data_points': len(period_data)
            }

        if not period_scores:
            logger.warning(f"No valid periods found for campaign {campaign_id}, defaulting to 14 days")
            return 14

        # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–∏–æ–¥ —Å –ª—É—á—à–∏–º score
        optimal_period = max(period_scores.keys(), key=lambda x: period_scores[x]['total_score'])

        logger.info(f"Adaptive period for campaign {campaign_id}: {optimal_period} days "
                   f"(score: {period_scores[optimal_period]['total_score']:.3f})")

        return optimal_period

    def _create_ensemble(self, period_predictions: Dict[str, SaturationPrediction],
                        target_spends: List[float]) -> EnsemblePrediction:
        """
        –°–æ–∑–¥–∞–µ—Ç ensemble –ø—Ä–æ–≥–Ω–æ–∑ –∏–∑ –≤—Å–µ—Ö –ø–µ—Ä–∏–æ–¥–æ–≤

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç weighted voting –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        - Confidence –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        - –ö–∞—á–µ—Å—Ç–≤–∞ fit (R¬≤)
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        - –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """

        logger.debug("Creating ensemble prediction from period predictions")

        # 1. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        period_weights = {}
        total_weight = 0

        for period_name, prediction in period_predictions.items():
            # –í–µ—Å = confidence * –∫–∞—á–µ—Å—Ç–≤–æ_fit * log(data_points)
            weight = (
                prediction.confidence * 0.4 +
                prediction.r_squared * 0.3 +
                min(np.log10(prediction.data_points) / 2, 1.0) * 0.2 +
                (1 if prediction.fit_quality in ['excellent', 'good'] else 0.5) * 0.1
            )

            period_weights[period_name] = weight
            total_weight += weight

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
        for period_name in period_weights:
            period_weights[period_name] /= total_weight

        logger.info(f"Ensemble weights: {period_weights}")

        # 2. –°–æ–∑–¥–∞–µ–º weighted consensus trajectory
        consensus_trajectory = []
        confidence_intervals = []

        for i, spend in enumerate(target_spends):
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ spend level
            predictions = []
            weights = []

            for period_name, prediction in period_predictions.items():
                pred_cpi = prediction.spend_trajectory[i]['predicted_cpi']
                predictions.append(pred_cpi)
                weights.append(period_weights[period_name])

            # Weighted average
            consensus_cpi = np.average(predictions, weights=weights)

            # Confidence interval (weighted std)
            variance = np.average((np.array(predictions) - consensus_cpi) ** 2, weights=weights)
            std_dev = np.sqrt(variance)

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º saturation level –¥–ª—è consensus
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏–∑ –≤—Å–µ—Ö saturation levels
            saturation_levels = [pred.spend_trajectory[i]['saturation_level']
                               for pred in period_predictions.values()]
            consensus_saturation = np.mean(saturation_levels)

            consensus_trajectory.append({
                'spend': spend,
                'predicted_cpi': consensus_cpi,
                'saturation_level': consensus_saturation,
                'efficiency_rating': self._rate_efficiency(consensus_saturation),
                'individual_predictions': {
                    period: pred.spend_trajectory[i]['predicted_cpi']
                    for period, pred in period_predictions.items()
                }
            })

            confidence_intervals.append({
                'spend': spend,
                'lower_bound': consensus_cpi - 1.96 * std_dev,
                'upper_bound': consensus_cpi + 1.96 * std_dev,
                'std_deviation': std_dev,
                'variance': variance
            })

        # 3. –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ ensemble
        ensemble_confidence = np.mean([pred.confidence for pred in period_predictions.values()])
        prediction_variance = np.mean([ci['variance'] for ci in confidence_intervals])

        # 4. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommended_spend = self._find_optimal_spend(consensus_trajectory)
        risk_level = self._assess_ensemble_risk(consensus_trajectory, confidence_intervals)

        return EnsemblePrediction(
            consensus_trajectory=consensus_trajectory,
            confidence_intervals=confidence_intervals,
            period_weights=period_weights,
            ensemble_confidence=ensemble_confidence,
            prediction_variance=prediction_variance,
            recommended_spend=recommended_spend,
            risk_level=risk_level
        )

    # === UTILITY METHODS ===

    def _get_historical_data(self, campaign_id: str) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏ –∏–∑ ClickHouse"""
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å –∫ ClickHouse
        # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç SQL –∑–∞–ø—Ä–æ—Å –∫ campaign_performance —Ç–∞–±–ª–∏—Ü–µ
        pass

    def _filter_by_period(self, data: pd.DataFrame, days: int) -> pd.DataFrame:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º N –¥–Ω—è–º"""
        cutoff_date = datetime.now() - timedelta(days=days)
        return data[data['date'] >= cutoff_date]

    def _fit_logistic_curve(self, spend_values: np.ndarray, cpi_values: np.ndarray) -> Tuple[float, float, float]:
        """Fit logistic curve: CPI = L / (1 + exp(-k * (spend - x0)))"""

        def logistic(x, L, k, x0):
            return L / (1 + np.exp(-k * (x - x0)))

        # Initial parameter estimates
        L_init = np.max(cpi_values) * 1.5  # Max CPI estimate
        k_init = 1 / np.std(spend_values)   # Steepness
        x0_init = np.mean(spend_values)     # Inflection point

        # Fit curve
        popt, _ = curve_fit(
            logistic,
            spend_values,
            cpi_values,
            p0=[L_init, k_init, x0_init],
            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]),
            maxfev=1000
        )

        return popt

    def _logistic_function(self, x: float, L: float, k: float, x0: float) -> float:
        """Logistic function"""
        return L / (1 + np.exp(-k * (x - x0)))

    def _calculate_saturation_level(self, spend: float, max_cpa: float, steepness: float, inflection_point: float) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –Ω–∞—Å—ã—â–µ–Ω–∏—è (0-1)"""
        current_cpa = self._logistic_function(spend, max_cpa, steepness, inflection_point)
        return current_cpa / max_cpa

    def _rate_efficiency(self, saturation_level: float) -> str:
        """–û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ spend level"""
        if saturation_level < 0.3:
            return "highly_efficient"
        elif saturation_level < 0.6:
            return "efficient"
        elif saturation_level < 0.8:
            return "moderate"
        else:
            return "inefficient"

    def _find_optimal_spend(self, trajectory: List[Dict]) -> float:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π spend (–ª—É—á—à–∏–π ROI point)"""
        # –ò—â–µ–º —Ç–æ—á–∫—É –≥–¥–µ efficiency –µ—â–µ good, –Ω–æ –±–ª–∏–∑–∫–æ –∫ —Å–Ω–∏–∂–µ–Ω–∏—é
        for item in trajectory:
            if item['saturation_level'] > 0.65:  # Before heavy saturation
                return item['spend']

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π efficient
        return trajectory[-1]['spend'] if trajectory else 0

    def _calculate_confidence(self, mape: float, r_squared: float, data_points: int, period_days: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç confidence score (0-1)"""

        # –ë–∞–∑–æ–≤—ã–π confidence –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
        mape_score = max(0, 1 - mape / 50)  # MAPE 0% = 1.0, 50% = 0.0
        r2_score = max(0, r_squared)

        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        data_bonus = min(data_points / 15, 1.0)  # Optimal 15+ points

        # Penalty –∑–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–∏–æ–¥ (–º–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π)
        period_penalty = 1.0 if period_days >= 14 else 0.8

        confidence = (mape_score * 0.4 + r2_score * 0.4 + data_bonus * 0.2) * period_penalty

        return min(max(confidence, 0.1), 1.0)  # Clip –º–µ–∂–¥—É 0.1 –∏ 1.0

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
def demo_multi_period_prediction():
    """–î–µ–º–æ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–æ–¥–µ–ª—å"""

    model = MultiPeriodSaturationModel()

    # –¶–µ–ª–µ–≤—ã–µ spend levels –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
    target_spends = [5000, 10000, 15000, 20000, 30000, 50000]

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–∞–º–ø–∞–Ω–∏–∏
    results = model.predict_multi_period("campaign_12345", target_spends)

    print("üöÄ Multi-Period Saturation Prediction Results")
    print("=" * 60)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
    for period_name, prediction in results['predictions_by_period'].items():
        print(f"\nüìä {period_name.upper()} PERIOD ({prediction.period_days} days)")
        print(f"   Confidence: {prediction.confidence:.1%}")
        print(f"   Data points: {prediction.data_points}")
        print(f"   Fit quality: {prediction.fit_quality}")
        print(f"   MAPE: {prediction.mape:.1%}")
        print(f"   Optimal spend: ${prediction.optimal_spend:,.0f}")

        if prediction.saturation_warning:
            print(f"   ‚ö†Ô∏è  Warning: {prediction.saturation_warning}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º ensemble —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if results['ensemble']:
        ensemble = results['ensemble']
        print(f"\nüéØ ENSEMBLE PREDICTION")
        print(f"   Ensemble confidence: {ensemble.ensemble_confidence:.1%}")
        print(f"   Recommended spend: ${ensemble.recommended_spend:,.0f}")
        print(f"   Risk level: {ensemble.risk_level}")

        print(f"\n   Period weights:")
        for period, weight in ensemble.period_weights.items():
            print(f"   - {period}: {weight:.1%}")

    return results

if __name__ == "__main__":
    demo_multi_period_prediction()
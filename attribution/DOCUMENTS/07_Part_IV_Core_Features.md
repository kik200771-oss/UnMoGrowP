# PART IV: CORE FEATURES & CAPABILITIES

**Parent Document:** 07_Complete_Technical_Specification_v1.0.md

---

## 4.1. Attribution Engine

### Overview

The attribution engine is the heart of the platform, responsible for matching user installs to marketing touchpoints (clicks, impressions, view-throughs) with high accuracy and low latency.

### Attribution Methods

**1. Deterministic Attribution** (Highest Priority)

```yaml
Method: Device ID Matching
Accuracy: 99.9%
Latency: <5ms

Process:
  1. Install event received with IDFA/GAID
  2. Query clicks table for matching device_id
  3. Apply attribution window (7 days click, 1 day view)
  4. Return first touch OR last touch (configurable)

Supported IDs:
  - IDFA (iOS)
  - GAID (Google Advertising ID)
  - IDFV (iOS Vendor ID, fallback)
  - Android ID (fallback)
  - Custom IDs (server-to-server)
```

```rust
// Rust implementation
pub async fn deterministic_match(
    install: &Install,
    pool: &Pool,
) -> Option<Attribution> {
    let query = "
        SELECT * FROM clicks
        WHERE device_id = ?
          AND timestamp > ?
          AND timestamp < ?
        ORDER BY timestamp ASC
        LIMIT 1
    ";

    let click = pool
        .query(query)
        .bind(&install.device_id)
        .bind(install.timestamp - CLICK_WINDOW)
        .bind(install.timestamp)
        .fetch_optional()
        .await?;

    Some(Attribution {
        install_id: install.id,
        click_id: click.id,
        method: Method::Deterministic,
        confidence: 1.0,
        attribution_time: install.timestamp - click.timestamp,
    })
}
```

**2. Referrer Matching** (iOS Universal Links, Android Referrer)

```yaml
Method: Deep Link / Referrer Matching
Accuracy: 95%+
Latency: <10ms

iOS Universal Links:
  - App receives universal link on install
  - Extract campaign parameters
  - Match to click in database
  - Verify timestamp within window

Android Referrer:
  - Google Play Install Referrer API
  - INSTALL_REFERRER broadcast
  - Extract UTM parameters
  - Match to campaign

Parameters:
  - campaign_id
  - publisher_id
  - creative_id
  - click_id (for exact matching)
```

**3. Probabilistic Attribution** (Fingerprinting)

```yaml
Method: Device Fingerprinting + ML
Accuracy: 70-85%
Latency: <50ms

Features (20+):
  - IP address
  - User-Agent string
  - Screen resolution
  - Timezone
  - Language
  - ISP/Carrier
  - Connection type
  - OS version
  - Device model
  - Browser (web)

Algorithm:
  1. Generate fingerprint hash
  2. Find clicks with similar fingerprints
  3. Calculate similarity score (0-1)
  4. ML model predicts match probability
  5. Threshold: >0.7 for attribution

ML Model:
  - Type: Gradient Boosting (XGBoost)
  - Features: 20+ device signals
  - Training: 10M+ labeled examples
  - Accuracy: 85% precision, 78% recall
```

**4. Self-Reporting Networks (SRN)**

```yaml
Method: API integration with ad networks
Accuracy: 95%+ (platform-reported)
Latency: <100ms

Supported:
  - Facebook Ads (Marketing API)
  - Google Ads (Conversion Upload)
  - TikTok Ads (Events API)
  - Snapchat Ads (Conversions API)
  - Apple Search Ads

Process:
  1. Install event sent to our platform
  2. We send install back to ad network API
  3. Ad network validates and returns attribution
  4. We record both our attribution + SRN attribution
  5. Reconciliation (weekly reports)

Benefits:
  - Platform-reported numbers (for parity)
  - SKAN support (iOS ATT)
  - No SDK integration needed
```

**5. SKAdNetwork (iOS 14+ Privacy)**

```yaml
Method: Apple's privacy-preserving attribution
Accuracy: Campaign-level only (no user-level)
Latency: 0-24 hour delay

How it works:
  1. User clicks ad
  2. Ad network receives "signed install"
  3. 0-24 hour timer starts (randomized)
  4. Postback sent with:
     - Campaign ID
     - Conversion value (0-63)
     - Did-win flag
  5. We aggregate SKAdNetwork data

Limitations:
  - No user-level data
  - Campaign-level only
  - 24h delay
  - 100 campaign limit per app

Our approach:
  - Combine SKAdNetwork + probabilistic
  - ML model to fill gaps
  - Aggregate reporting for iOS 14+
```

### Multi-Touch Attribution

```yaml
Models Supported:
  - First Touch: Credit to first click
  - Last Touch: Credit to last click (industry standard)
  - Linear: Equal credit to all touches
  - Time Decay: More credit to recent touches
  - U-Shaped: 40% first, 40% last, 20% middle
  - W-Shaped: 30% first, 30% last, 40% middle
  - Data-Driven: ML model learns optimal weights

Implementation:
  - Store all touches (clicks, impressions, views)
  - On install, query all touches within window
  - Apply credit allocation model
  - Write to attribution_touches table
  - Aggregate for reporting
```

```sql
-- Multi-touch attribution query
WITH user_touches AS (
    SELECT
        user_id,
        touch_type,
        publisher_id,
        timestamp,
        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as touch_number,
        COUNT(*) OVER (PARTITION BY user_id) as total_touches
    FROM touches
    WHERE user_id = ?
      AND timestamp BETWEEN ? AND ?
)
SELECT
    publisher_id,
    CASE attribution_model
        WHEN 'first_touch' THEN IF(touch_number = 1, 1.0, 0.0)
        WHEN 'last_touch' THEN IF(touch_number = total_touches, 1.0, 0.0)
        WHEN 'linear' THEN 1.0 / total_touches
        WHEN 'time_decay' THEN EXP(-0.7 * (total_touches - touch_number))
    END as credit
FROM user_touches;
```

### Cross-Device Attribution

```yaml
Challenge: User sees ad on mobile, installs on tablet
Solution: Device graph + probabilistic matching

Identity Graph (Neo4j):
  Nodes:
    - Devices (IDFA, GAID, etc.)
    - Users (deterministic ID from login)
    - IPs
    - Households (same WiFi)

  Edges:
    - same_user (deterministic link)
    - same_household (probabilistic)
    - same_network (IP-based)

Algorithm:
  1. Install on Device B
  2. No direct click match
  3. Query identity graph for connected devices
  4. Find Device A with matching click
  5. Calculate confidence based on:
     - Time delta
     - Graph distance
     - Signal strength
  6. Attribute if confidence > 0.8
```

```cypher
// Neo4j query for cross-device matching
MATCH (install_device:Device {id: $device_id})
  -[:SAME_USER|SAME_HOUSEHOLD*1..2]-
  (click_device:Device)
  -[:HAS_CLICK]-
  (click:Click)
WHERE click.timestamp > install_device.timestamp - 7 days
  AND click.timestamp < install_device.timestamp
RETURN click, click_device,
       length(path) as distance,
       (1.0 / length(path)) as confidence
ORDER BY confidence DESC, click.timestamp DESC
LIMIT 1
```

### Attribution Windows

```yaml
Defaults (Configurable per Campaign):
  Click-Through Window: 7 days
  View-Through Window: 1 day (24 hours)
  Re-engagement Window: 30 days

Explanation:
  - Click-Through: User clicked ad, installed within 7 days
  - View-Through: User saw ad (impression), installed within 1 day
  - Re-engagement: Existing user, re-opened app after 30 days inactivity

Why these windows:
  - 7 days click: Industry standard (covers consideration period)
  - 1 day view: Prevents over-attribution (views are passive)
  - 30 days re-engagement: Balances credit vs attribution inflation

Customization:
  - Gaming apps: Shorter windows (3 days click, 6h view)
  - E-commerce: Longer windows (14 days click, 3 days view)
  - Travel apps: Very long windows (30 days click, 7 days view)
```

### Deduplication

```yaml
Challenge: Same event sent multiple times (network retries, duplicate SDKs)
Solution: Redis-based dedup with 24h TTL

Process:
  1. Event received with event_id
  2. Check Redis: EXISTS dedup:{event_id}
  3. If exists: Return 200 OK (idempotent), discard event
  4. If not exists:
     - SET dedup:{event_id} 1 EX 86400
     - Process event normally

TTL: 24 hours (balances memory vs late duplicates)

Key format: dedup:{event_type}:{event_id}
Examples:
  - dedup:install:abc123
  - dedup:click:xyz789
  - dedup:purchase:def456
```

### Performance Targets

```yaml
Latency (P95):
  Deterministic: <10ms
  Probabilistic: <50ms
  SRN: <100ms
  Cross-device: <200ms

Throughput:
  Clicks: 100K/second
  Installs: 10K/second
  Attributions: 10K/second

Accuracy:
  Deterministic: 99.9%
  Probabilistic: 85%
  Overall: 95%+ (weighted)

Availability:
  SLA: 99.99% (52 minutes downtime/year)
  Multi-region: Active-active
  Failover: <30 seconds
```

---

## 4.2. Fraud Detection System

### Overview

**5-Layer Defense System** with zero conflict of interest (we don't bill per install, so detecting fraud helps us AND customers).

### Layer 1: Real-Time Rule Engine (Rust)

```yaml
Purpose: Catch obvious fraud patterns instantly
Language: Rust (for speed)
Latency: <10ms
Location: Ingestion service (inline)

Rules (50+):
  Temporal Anomalies:
    - CTIT < 2 seconds (Click-to-Install-Time too fast)
    - CTIT > 30 days (too slow, likely organic)
    - Install timestamp in future
    - Install timestamp too old (>90 days)

  Device Anomalies:
    - Impossible device (iOS with Android UA)
    - Emulator detected (BlueStacks, Genymotion)
    - Rooted/Jailbroken device (security risk)
    - Developer mode enabled
    - Mock location enabled

  Network Anomalies:
    - VPN/Proxy detected (IP reputation)
    - Datacenter IP (not residential)
    - IP blacklisted (fraud database)
    - Geo-velocity (impossible travel)
      Example: Click in US, install in China 1 min later

  Behavioral Anomalies:
    - No user interaction (SDK initialized, no events)
    - Suspicious event sequence (purchase before onboarding)
    - Macro-like behavior (robotic patterns)

Action:
  - Flag event with fraud_risk: 'high'
  - Continue processing (not blocking yet)
  - Send to Layer 2 for ML scoring
```

```rust
// Rust rule engine
pub struct RuleEngine {
    ip_reputation: IpReputationService,
    device_detector: DeviceDetectorService,
}

impl RuleEngine {
    pub fn check(&self, event: &Event) -> RiskLevel {
        let mut risk_score = 0.0;

        // CTIT check
        if let Some(click) = event.click {
            let ctit = event.timestamp - click.timestamp;
            if ctit < Duration::seconds(2) {
                risk_score += 0.8; // Very suspicious
            }
        }

        // IP reputation
        let ip_score = self.ip_reputation.check(&event.ip);
        if ip_score < 30 {
            risk_score += 0.6; // Bad IP
        }

        // Device check
        if self.device_detector.is_emulator(&event.user_agent) {
            risk_score += 0.9; // Emulator = high risk
        }

        // Classify
        if risk_score > 1.5 { RiskLevel::High }
        else if risk_score > 0.8 { RiskLevel::Medium }
        else { RiskLevel::Low }
    }
}
```

### Layer 2: ML Ensemble (4 Models)

**Model 1: XGBoost Classifier**

```yaml
Purpose: Binary classification (fraud vs legit)
Type: Gradient Boosting Decision Trees
Features: 150+ engineered features
Training data: 50M+ labeled events (historical)
Performance:
  - Precision: 94%
  - Recall: 89%
  - F1: 91.5%
  - Latency: <10ms

Feature Categories (150+ total):
  Temporal (20):
    - CTIT (Click-to-Install-Time)
    - Hour of day, day of week
    - Time since app launch
    - Session duration
    - Events per session
    - Time to first event
    - Time to first revenue event

  Device (30):
    - Device model, OS version
    - Screen resolution, DPI
    - Battery level, storage available
    - RAM, CPU cores
    - Device language, timezone
    - Device fingerprint (hashed)

  Network (25):
    - IP address (hashed)
    - IP geolocation (country, city, lat/lon)
    - ISP/Carrier
    - Connection type (WiFi, Cellular, VPN)
    - VPN detection score
    - IP reputation score
    - ASN (Autonomous System Number)

  Behavioral (40):
    - Click patterns (clicks per minute)
    - Navigation patterns
    - Scroll depth (web)
    - Mouse movement entropy
    - Touch pressure variance (mobile)
    - Event sequences
    - Funnel progression speed

  Campaign (20):
    - Publisher ID
    - Historical fraud rate for publisher
    - Campaign ID
    - Ad format
    - Site ID, Sub-site ID

  Aggregated (15):
    - Installs from this IP last 24h
    - Installs from this device model last 24h
    - Publisher fraud rate last 7d
    - Network quality score

Training:
  - Daily retraining (incremental)
  - Optuna hyperparameter tuning
  - Cross-validation (5-fold)
  - Active learning (human feedback loop)
```

**Model 2: LSTM Neural Network**

```yaml
Purpose: Detect sequential patterns (bot-like behavior)
Type: Long Short-Term Memory (Recurrent NN)
Input: Sequence of last 100 events per user/device
Performance:
  - Precision: 88%
  - Recall: 82%
  - Latency: <50ms

Architecture:
  Input: (100 timesteps, 50 features per event)
    â†“
  LSTM Layer (128 units, dropout 0.3)
    â†“
  LSTM Layer (64 units, dropout 0.3)
    â†“
  Dense Layer (32 units, ReLU)
    â†“
  Output (1 unit, Sigmoid) â†’ Fraud probability

What it catches:
  - Robotic event patterns
  - Macro-driven behavior
  - Bot farms (identical sequences)
  - Click spam (repeated patterns)

Example:
  Legit user: [click, wait 15s, install, wait 30s, open, explore, ...]
  Bot: [click, wait 0.5s, install, wait 0.5s, open, wait 0.5s, ...]
  LSTM detects: "Too regular, suspicious"
```

**Model 3: Isolation Forest**

```yaml
Purpose: Unsupervised anomaly detection
Type: Ensemble of decision trees
Input: Same 150+ features
Performance:
  - Anomaly detection rate: 93%
  - False positive rate: 7%
  - Latency: <5ms

Why important:
  - Catches NEW fraud types (not seen in training)
  - No labels needed (unsupervised)
  - Adapts to evolving fraud

Training:
  - Weekly retraining on recent data (last 7 days)
  - Contamination parameter: 0.05 (assume 5% fraud)

What it catches:
  - Novel fraud patterns
  - Zero-day fraud techniques
  - Outlier behavior (statistical anomalies)
```

**Model 4: Graph Neural Network (GNN)**

```yaml
Purpose: Detect fraud networks (coordinated fraud)
Type: Graph Convolutional Network
Input: Graph of devices, IPs, publishers
Performance:
  - Network detection rate: 89%
  - Latency: Batch processing (not real-time)

Graph Structure:
  Nodes:
    - Devices (100M+)
    - IPs (10M+)
    - Publishers (50K+)
    - Apps (5K+)

  Edges:
    - same_ip (device â†’ device)
    - same_publisher (device â†’ device)
    - attributed_to (install â†’ publisher)
    - fraud_connection (weighted by suspicion)

Algorithm:
  1. Build subgraph around suspicious event
  2. GNN propagates fraud scores through graph
  3. Identify connected components (fraud rings)
  4. Score entire subgraph

Example detection:
  - 100 devices, all from same IP
  - All installed same app within 1 hour
  - All from same publisher
  - GNN identifies: "Fraud ring"

Training:
  - Weekly (computationally expensive)
  - GraphSAGE architecture
  - PyTorch Geometric
```

**Ensemble Voting**

```yaml
Method: Weighted average + Meta-learner

Weights (learned from validation data):
  - XGBoost: 0.40 (highest weight)
  - LSTM: 0.30
  - Isolation Forest: 0.20
  - GNN: 0.10 (batch only)

Formula:
  fraud_score = 0.4 * xgb_score
              + 0.3 * lstm_score
              + 0.2 * isolation_score
              + 0.1 * gnn_score

Meta-learner:
  - Type: Logistic Regression
  - Input: 4 model outputs + features
  - Output: Final fraud probability (0-1)
  - Trained on validation set

Final score: 0-100 (scaled for UI)
  - 0-30: Low risk (green)
  - 31-70: Medium risk (yellow)
  - 71-100: High risk (red)
```

### Layer 3: Collaborative Intelligence (Federated Learning)

```yaml
Concept: Share fraud patterns across customers without sharing raw data

How it works:
  Customer A trains model on their data
    â†“
  Extract model gradients (not data)
    â†“
  Send gradients to central server (encrypted)
    â†“
  Server aggregates gradients from all customers
    â†“
  Update global model
    â†“
  Distribute global model back to customers

Privacy guarantees:
  - Differential privacy (add noise to gradients)
  - Secure aggregation (homomorphic encryption)
  - No raw data leaves customer premises
  - GDPR compliant

Benefits:
  - Network effect: Fraud detected at A protects B
  - Faster detection of new fraud types
  - Better models (trained on more data)
  - Privacy preserved

Example:
  - Customer A sees new click injection pattern
  - Model learns to detect it
  - Gradients shared (anonymized)
  - Customer B's model updated
  - Customer B protected from same attack
```

### Layer 4: Behavioral Analysis

```yaml
Purpose: Detect bots and non-human behavior

Bot Detection:
  Mouse/Touch Patterns:
    - Mouse movement entropy (randomness)
    - Touch pressure variance
    - Scroll behavior (smooth vs jerky)
    - Interaction timing (human-like vs robotic)

  Browser Signals (Web):
    - Canvas fingerprinting
    - WebGL fingerprinting
    - Font detection
    - Audio context
    - Browser plugins
    - Screen resolution

  Mobile Signals:
    - Gyroscope/Accelerometer data
    - Touch multi-point patterns
    - App switch behavior
    - Background app usage

SDK Integrity:
  - Certificate pinning (prevent tampering)
  - Code obfuscation (anti-reverse-engineering)
  - Runtime Application Self-Protection (RASP)
  - Checksum verification
  - Anti-debugging detection

RASP (Runtime Application Self-Protection):
  - Detect if SDK modified
  - Detect if running in emulator
  - Detect if debugger attached
  - Detect if traffic intercepted (proxy)
  - Self-destruct if compromised
```

### Layer 5: Post-Attribution Verification

```yaml
Purpose: Validate attribution AFTER user starts using app

Signals:
  Engagement Validation:
    - Did user complete onboarding?
    - Number of sessions in first 7 days
    - Session duration (>30s?)
    - Feature usage (explored app?)
    - Content consumed

  LTV Prediction:
    - Predicted LTV from Day 1 behavior
    - If predicted LTV << campaign average â†’ suspicious
    - If predicted LTV = $0 â†’ likely fraud

  Cohort Analysis:
    - Compare user cohort to historical
    - If cohort performs 50%+ worse â†’ investigate
    - If zero retention â†’ fraud

Actions:
  If fraud detected after attribution:
    - Mark install as "suspected fraud"
    - Don't charge customer (refund if already billed)
    - Blacklist publisher/network
    - Update ML models with new label
```

### Explainable AI (XAI) - SHAP Values

```yaml
Purpose: Explain WHY an event was flagged as fraud

Technology: SHAP (SHapley Additive exPlanations)

Process:
  1. ML model predicts fraud score
  2. SHAP calculates feature contributions
  3. Rank features by impact

Output:
  Fraud Score: 87% (High Risk)

  Top Contributing Factors:
    ðŸ”´ CTIT: 0.3 seconds (-0.43 contribution)
       â†’ Humans take 5-30s, this is bot-like

    ðŸ”´ IP Reputation: 23/100 (-0.31 contribution)
       â†’ Known VPN/proxy, datacenter IP

    ðŸ”´ Click Frequency: 47 clicks/min (-0.28 contribution)
       â†’ Normal: 1-5 clicks/min, this is spam

    ðŸŸ¡ Device Age: 2 days old (+0.12 contribution)
       â†’ Slightly suspicious (new device)

    ðŸŸ¢ User Behavior: Normal session duration (+0.08)
       â†’ User spent reasonable time in app

  Recommendation: Block this install, flag publisher for review

Benefits:
  - Transparency (users understand decisions)
  - Trust (not "black box")
  - Debugging (analysts verify correctness)
  - Compliance (GDPR requires explainability)
  - Model improvement (insights for feature engineering)
```

### Fraud Prevention Actions

```yaml
Real-Time Actions:
  Low Risk (0-30):
    - Allow through
    - Standard attribution

  Medium Risk (31-70):
    - Allow through
    - Mark as "review_needed"
    - Human review queue (analysts check samples)
    - Don't block (false positive risk)

  High Risk (71-100):
    - Block attribution (don't credit publisher)
    - Flag for customer review
    - Blacklist if pattern confirmed
    - Send alert to customer
    - Don't charge customer

Post-Processing:
  - Daily batch analysis of all events
  - GNN fraud network detection
  - Publisher fraud rate calculation
  - Weekly fraud reports
  - Quarterly publisher review

Publisher Management:
  - Fraud rate < 5%: Green (good)
  - Fraud rate 5-15%: Yellow (warning)
  - Fraud rate > 15%: Red (suspend)
  - Fraud rate > 30%: Blacklist (permanent)

Customer Controls:
  - Adjust thresholds (risk tolerance)
  - Whitelist/blacklist publishers
  - Custom fraud rules
  - Auto-block vs manual review
```

### Performance & Accuracy

```yaml
Latency:
  Layer 1 (Rules): <10ms
  Layer 2 (ML): <35ms
  Total: <50ms (P95)

Accuracy (vs independent audit):
  Precision: 95% (of flagged fraud, 95% is truly fraud)
  Recall: 92% (of all fraud, we catch 92%)
  F1 Score: 93.5%
  False Positive Rate: 5%

Industry Comparison:
  Our platform: 95% precision, 92% recall
  AppsFlyer/Adjust: ~50-60% (conflict of interest)
  Independent fraud companies: 90%+ (but cost extra)

Business Impact (for $1M/month ad spend):
  Fraud rate: 8% industry average
  Fraud amount: $80K/month
  Our detection: 92% catch rate = $73.6K saved
  Competitor: 50% catch rate = $40K saved
  Difference: $33.6K/month more savings with us
```

---

*[Document continues in next file: Part V - AI/ML Intelligence System]*